from typing import List, Dict, Any
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import re
import logging
import requests
import random
from urllib.parse import urlparse, parse_qs
import traceback

# Get module logger
logger = logging.getLogger("job_search_app.scraper")

# List of user agents to randomize for better anonymity
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15",
]

def scrape_with_retry(driver, url, parser_type, retry_num=0, max_retries=2):
    """Scrape job listings with retry capability and enhanced interaction methods"""
    logger.info(f"Attempting to scrape {url} with {parser_type} parser (attempt {retry_num+1}/{max_retries+1})")
    
    try:
        # Navigate to the URL with a random delay to mimic human behavior
        driver.get(url)
        time.sleep(random.uniform(1.5, 3.0))  # Random delay to appear more human-like
        
        # Wait for page to load with different strategies based on site
        if 'linkedin.com' in url.lower():
            # LinkedIn-specific waiting strategy
            try:
                # Wait for one of multiple possible selectors - LinkedIn has several job card formats
                selectors = [
                    ".base-search-card", 
                    ".job-search-card", 
                    ".jobs-search-results__list-item", 
                    ".job-card", 
                    "div[data-job-id]", 
                    "li[data-occludable-job-id]"
                ]
                
                for selector in selectors:
                    try:
                        element = WebDriverWait(driver, 5).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                        )
                        logger.info(f"LinkedIn job cards found using selector: {selector}")
                        break
                    except:
                        continue
                
                logger.info("LinkedIn page loaded, implementing progressive scrolling for lazy-loaded content...")
                
                # Execute multiple slow scrolls to ensure lazy-loaded content is visible
                # This better mimics human behavior and ensures all content loads
                for i in range(4):
                    scroll_height = driver.execute_script("return document.body.scrollHeight")
                    for step in range(4):  # Break each scroll into multiple smaller movements
                        current_position = scroll_height * (i * 4 + step) / 16
                        driver.execute_script(f"window.scrollTo(0, {current_position});")
                        time.sleep(random.uniform(0.3, 0.7))  # Random delay between micro-scrolls
                    
                    # Small pause at each quarter of the page
                    time.sleep(random.uniform(0.8, 1.5))
                    
                    # Try clicking "Show more jobs" button if present
                    try:
                        show_more = driver.find_element(By.XPATH, "//button[contains(text(), 'Show more')]")
                        show_more.click()
                        time.sleep(1)
                        logger.info("Clicked 'Show more jobs' button")
                    except:
                        pass
                    
                # Final scroll to bottom with a bit more time to load content
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2.5)
                
            except Exception as e:
                logger.warning(f"Timed out waiting for LinkedIn job cards: {str(e)}")
                # Try a more generic approach
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
                time.sleep(3)
                
        elif 'indeed.com' in url.lower():
            # Enhanced Indeed-specific waiting strategy
            try:
                # Wait for any of multiple possible job card selectors
                selectors = [
                    ".job_seen_beacon", 
                    ".jobsearch-ResultsList", 
                    ".mosaic-provider-jobcards", 
                    ".job_component_wrapper",
                    ".jobsearch-SerpJobCard", 
                    ".tapItem"
                ]
                
                for selector in selectors:
                    try:
                        element = WebDriverWait(driver, 5).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                        )
                        logger.info(f"Indeed job cards found using selector: {selector}")
                        break
                    except:
                        continue
                
                # Progressive scrolling for better content loading
                logger.info("Indeed page loaded, implementing progressive scrolling...")
                
                # Accept cookies if the popup appears
                try:
                    cookie_button = WebDriverWait(driver, 3).until(
                        EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
                    )
                    cookie_button.click()
                    logger.info("Accepted cookies on Indeed")
                    time.sleep(0.5)
                except:
                    pass
                    
                # Close any modal that might appear
                try:
                    close_button = driver.find_element(By.CSS_SELECTOR, "[aria-label='Close']")
                    close_button.click()
                    logger.info("Closed popup modal on Indeed")
                except:
                    pass
                
                # Execute multiple slow scrolls with random pauses
                total_height = driver.execute_script("return document.body.scrollHeight")
                for i in range(5):
                    scroll_position = total_height * (i + 1) / 6
                    driver.execute_script(f"window.scrollTo(0, {scroll_position});")
                    time.sleep(random.uniform(0.7, 1.5))  # Random pause to mimic human reading
                    
                    # Try clicking "Next" or "Show more jobs" buttons if present
                    try:
                        next_button = driver.find_element(By.XPATH, "//a[contains(text(), 'Next') or contains(text(), 'Show more')]")
                        if next_button.is_displayed() and next_button.is_enabled():
                            next_button.click()
                            logger.info("Clicked navigation button to load more jobs")
                            time.sleep(2)  # Wait for new content to load
                    except:
                        pass
                
                # Final scroll to ensure all content is loaded
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2.5)
                
            except Exception as e:
                logger.warning(f"Timed out waiting for Indeed job cards: {str(e)}")
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(3)
                
        else:
            # Enhanced generic waiting strategy with multiple detection techniques
            logger.info(f"Using advanced generic waiting strategy for {url}")
            
            # Wait for the page to load basic structure
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(random.uniform(1.0, 2.0))
            
            # Handle common cookie consent banners that might block content
            try:
                for cookie_text in ['accept cookies', 'accept all', 'i accept', 'agree', 'got it']:
                    buttons = driver.find_elements(By.XPATH, f"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{cookie_text}')]")
                    for btn in buttons:
                        if btn.is_displayed():
                            btn.click()
                            logger.info(f"Accepted cookies with text: {cookie_text}")
                            time.sleep(0.5)
                            break
            except Exception as e:
                logger.debug(f"Error handling cookie consent: {e}")
            
            # Try to detect job listing elements with broader patterns
            job_found = False
            common_job_patterns = [
                "div[class*='job'], div[class*='card'], li[class*='result'], div[class*='listing']",
                "div[class*='post'], div[class*='vacancy'], div[class*='position'], div[class*='offer']",
                "article[class*='job'], article[class*='listing'], div[class*='opportunity']",
                "div.jobs, ul.jobs, div.results, ul.results, div.listings, ul.listings",
                "div[id*='job'], div[id*='result'], div[id*='listing']",
                "a[href*='job'], a[href*='career'], a[href*='position']"
            ]
            
            for pattern in common_job_patterns:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, pattern)
                    if elements:
                        logger.info(f"Found {len(elements)} potential job elements using pattern: {pattern}")
                        job_found = True
                        break
                except Exception:
                    pass
            
            if not job_found:
                logger.warning("Couldn't find specific job elements, continuing with basic page analysis")
            
            # Enhanced progressive scrolling with adaptive pauses
            # First get page height and determine number of scroll steps
            total_height = driver.execute_script("return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight)")
            viewport_height = driver.execute_script("return window.innerHeight")
            scroll_steps = min(max(int(total_height / viewport_height) + 1, 4), 8)
            
            logger.info(f"Starting progressive scroll with {scroll_steps} steps for a {total_height}px page")
            
            # Scroll down in increments with random pauses to mimic human behavior and trigger lazy loading
            for i in range(scroll_steps):
                scroll_fraction = (i + 1) / scroll_steps
                scroll_position = total_height * scroll_fraction
                
                # Smooth scroll with small increments
                current_position = driver.execute_script("return window.pageYOffset")
                target_position = int(scroll_position)
                step_size = max(int((target_position - current_position) / 10), 100)
                
                for step in range(current_position, target_position, step_size):
                    driver.execute_script(f"window.scrollTo(0, {step})")
                    time.sleep(0.05)  # Very short pause during smooth scroll
                
                # Final position for this increment
                driver.execute_script(f"window.scrollTo(0, {target_position})")
                
                # Adaptive pause based on position - spend more time in the middle of the page
                if i < scroll_steps - 1:  # Not last scroll
                    pause_time = random.uniform(0.7, 1.8)
                    if 0.3 < scroll_fraction < 0.7:  # Middle of page gets longer pauses
                        pause_time *= 1.5
                    time.sleep(pause_time)
                    
                    # Try to interact with any "Load more" or pagination elements
                    try:
                        for text_pattern in ['more', 'next', 'load', 'show', 'page']:
                            load_more_elements = driver.find_elements(By.XPATH, 
                                f"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{text_pattern}')] | " +
                                f"//a[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{text_pattern}')]")
                            
                            for element in load_more_elements:
                                if element.is_displayed() and element.is_enabled():
                                    element.click()
                                    logger.info(f"Clicked on an element containing '{text_pattern}'")
                                    time.sleep(2)  # Wait for new content
                                    break
                    except Exception as e:
                        logger.debug(f"Error interacting with page navigation: {e}")
            
            # Final scroll to very bottom and pause to let everything load
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2.5)

        # Ensure page has fully rendered by waiting a bit more
        time.sleep(random.uniform(0.8, 1.2))
        
        # Get the page source after all waiting and scrolling
        html_content = driver.page_source
        content_length = len(html_content)
        logger.info(f"Retrieved HTML content, length: {content_length} characters")
        
        # Validate we got meaningful content with multiple checks
        content_valid = True
        validation_messages = []
        
        # Check 1: Minimum content size
        if content_length < 5000:  # More conservative minimum size threshold
            content_valid = False
            msg = f"Retrieved HTML content is suspiciously small ({content_length} bytes)"
            validation_messages.append(msg)
            logger.warning(msg)
            
        # Check 2: Contains expected HTML structure
        if "<body" not in html_content or "</body>" not in html_content:
            content_valid = False
            msg = "HTML content lacks proper body tags"
            validation_messages.append(msg)
            logger.warning(msg)
            
        # Check 3: Make sure we got actual content, not an error page
        error_indicators = ['access denied', 'captcha', 'blocked', 'detected automated', 'too many requests']
        for indicator in error_indicators:
            if indicator in html_content.lower():
                content_valid = False
                msg = f"Potential blocking detected: page contains '{indicator}'"
                validation_messages.append(msg)
                logger.warning(msg)
                break
        
        # Check 4: Site-specific checks
        job_indicators_found = False
        if parser_type == 'linkedin':
            job_indicators = ['job-search-card', 'base-search-card', 'jobs-search-results']
            for indicator in job_indicators:
                if indicator in html_content:
                    job_indicators_found = True
                    break
        elif parser_type == 'indeed':
            job_indicators = ['job_seen_beacon', 'jobsearch-Results', 'mosaic-provider-jobcards']
            for indicator in job_indicators:
                if indicator in html_content:
                    job_indicators_found = True
                    break
        else:
            # For generic sites, look for common job listing terms
            job_indicators = ['job', 'career', 'position', 'vacancy', 'employment', 'hiring']
            for indicator in job_indicators:
                pattern = f'class=["\'].*?{indicator}.*?["\']'
                if re.search(pattern, html_content, re.IGNORECASE):
                    job_indicators_found = True
                    break
        
        if not job_indicators_found and (parser_type == 'linkedin' or parser_type == 'indeed'):
            content_valid = False
            msg = f"Could not find expected job indicators for {parser_type}"
            validation_messages.append(msg)
            logger.warning(msg)
        
        # If content validation failed, retry or raise error
        if not content_valid:
            if retry_num < max_retries:
                logger.info(f"Content validation failed: {'; '.join(validation_messages)}")
                logger.info(f"Retrying with attempt {retry_num+2}/{max_retries+1}")
                
                # Clear cookies and cache before retry
                driver.delete_all_cookies()
                driver.execute_script("window.localStorage.clear();")
                driver.execute_script("window.sessionStorage.clear();")
                
                # Wait before retrying
                time.sleep(random.uniform(3.0, 5.0))
                return scrape_with_retry(driver, url, parser_type, retry_num+1, max_retries)
            else:
                error_msg = f"Failed content validation after {max_retries+1} attempts: {'; '.join(validation_messages)}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
        return html_content
        
    except Exception as e:
        logger.error(f"Error during scraping attempt {retry_num+1}: {str(e)}")
        if retry_num < max_retries:
            logger.info(f"Retrying with attempt {retry_num+2}/{max_retries+1}")
            # Randomize wait time before retry
            time.sleep(random.uniform(2.0, 4.0))
            
            # Clear any alerts or popups that might be causing issues
            try:
                driver.switch_to.alert.dismiss()
            except:
                pass
                
            return scrape_with_retry(driver, url, parser_type, retry_num+1, max_retries)
        else:
            raise e

def scrape_jobs(url: str) -> List[Dict[str, Any]]:
    """
    Scrape job listings from the given URL with advanced anonymity and robust retry mechanisms
    
    Args:
        url: URL of the job listing page
        
    Returns:
        List of dictionaries containing job details
    """
    logger.info(f"Starting job scraping from URL: {url}")
    max_retries = 5  # Increased from 3 to 5 for more persistence
    retry_count = 0
    driver = None
    
    while retry_count < max_retries:
        try:
            # Configure enhanced Selenium options for better anonymity and reliability
            chrome_options = Options()
            
            # Change headless mode based on retry attempts
            # Sometimes headless mode is detected, so try with visible browser on later attempts
            if retry_count < 3:
                chrome_options.add_argument("--headless=new")  # New headless mode
                logger.info("Using headless mode")
            else:
                logger.info("Using visible browser mode to avoid detection")
            
            # Core browser settings
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            
            # Set realistic viewport
            width = random.choice([1280, 1366, 1440, 1536, 1600, 1920])
            height = random.choice([800, 864, 900, 1024, 1080])
            chrome_options.add_argument(f"--window-size={width},{height}")
            logger.info(f"Setting window size: {width}x{height}")
            
            # Enhanced anti-detection features
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            
            # Completely randomize user agent based on retry attempts for better IP rotation simulation
            if retry_count > 0:
                # Use more varied user agents on retry attempts
                user_agents_extended = [
                    # Windows Chrome
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
                    # Mac Chrome
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
                    # Windows Firefox
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0",
                    # Mac Firefox
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/114.0",
                    # Windows Edge
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.51",
                    # iOS Safari
                    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Mobile/15E148 Safari/604.1",
                    # Android Chrome
                    "Mozilla/5.0 (Linux; Android 13; SM-S908B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Mobile Safari/537.36",
                ] + USER_AGENTS
                user_agent = random.choice(user_agents_extended)
            else:
                user_agent = random.choice(USER_AGENTS)
                
            chrome_options.add_argument(f"--user-agent={user_agent}")
            logger.info(f"Using user agent: {user_agent}")
            
            # Additional advanced anonymity options
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option("useAutomationExtension", False)
            chrome_options.add_argument("--disable-extensions")
            
            # Randomize language and geolocation settings to appear more organic
            languages = ["en-US,en;q=0.9", "en-GB,en;q=0.9", "en-CA,en;q=0.9", "en;q=0.9"]
            chrome_options.add_argument(f"--lang={random.choice(languages)}")
            
            # Privacy and notification settings
            chrome_options.add_argument("--incognito")
            chrome_options.add_argument("--disable-notifications")
            chrome_options.add_argument("--disable-infobars")
            chrome_options.add_argument("--disable-popup-blocking")
            
            # Add proxy if available (commented out but could be enabled)
            # if retry_count > 1 and PROXIES:  # Use proxy on later retry attempts
            #     proxy = random.choice(PROXIES)
            #     chrome_options.add_argument(f'--proxy-server={proxy}')
            #     logger.info(f"Using proxy: {proxy}")
            
            logger.info(f"Setting up Chrome driver with enhanced anonymity options (attempt {retry_count+1}/{max_retries})")
            
            # Set up the Chrome driver with better error handling
            try:
                driver = webdriver.Chrome(
                    service=Service(ChromeDriverManager().install()),
                    options=chrome_options
                )
                # Extend page load timeout on later retries
                timeout = 30 + (retry_count * 10)  # Increase timeout with each retry
                driver.set_page_load_timeout(timeout)
                driver.set_script_timeout(timeout)
                logger.info(f"Set page load timeout to {timeout} seconds")
            except Exception as driver_error:
                logger.error(f"Error setting up Chrome driver: {str(driver_error)}")
                raise

            # Execute advanced CDP commands to bypass bot detection
            driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                "source": """
                // Override property detection
                const originalQuery = window.navigator.permissions.query;
                window.navigator.permissions.query = (parameters) => (
                    parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
                );
                
                // Mask WebDriver property
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                
                // Override navigator properties to make detection more difficult
                window.chrome = {
                    app: {
                        isInstalled: false,
                        InstallState: {
                            DISABLED: 'disabled',
                            INSTALLED: 'installed',
                            NOT_INSTALLED: 'not_installed'
                        },
                        RunningState: {
                            CANNOT_RUN: 'cannot_run',
                            READY_TO_RUN: 'ready_to_run',
                            RUNNING: 'running'
                        }
                    },
                    runtime: {
                        OnInstalledReason: {
                            INSTALL: 'install',
                            UPDATE: 'update',
                            CHROME_UPDATE: 'chrome_update',
                            SHARED_MODULE_UPDATE: 'shared_module_update'
                        },
                        OnRestartRequiredReason: {
                            APP_UPDATE: 'app_update',
                            OS_UPDATE: 'os_update',
                            PERIODIC: 'periodic'
                        }
                    }
                };
                
                // Add missing Chrome functions to prevent detection
                if (window.navigator.plugins) {
                    // If no plugins, add some fake ones
                    if (window.navigator.plugins.length === 0) {
                        Object.defineProperty(navigator, 'plugins', {
                            get: () => {
                                const plugins = [
                                    { name: 'Chrome PDF Plugin', filename: 'internal-pdf-viewer', description: 'Portable Document Format' },
                                    { name: 'Chrome PDF Viewer', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai', description: 'Portable Document Format' },
                                    { name: 'Native Client', filename: 'internal-nacl-plugin', description: '' }
                                ];
                                
                                return Object.setPrototypeOf({
                                    length: plugins.length,
                                    ...plugins.reduce((acc, p, i) => ({...acc, [i]: p}), {})
                                }, PluginArray.prototype);
                            }
                        });
                    }
                }
                
                // Add languages to mimic real browser
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['en-US', 'en'],
                });
                """
            })
            
            # Add additional CDP commands to modify browser fingerprint
            driver.execute_cdp_cmd("Network.enable", {})
            
            # Randomize user-agent at browser level as well (belt and suspenders approach)
            driver.execute_cdp_cmd("Network.setUserAgentOverride", {
                "userAgent": user_agent,
                "acceptLanguage": random.choice(["en-US,en;q=0.9", "en-GB,en;q=0.8,en-US;q=0.6"]),
                "platform": random.choice(["Windows NT 10.0; Win64; x64", "Macintosh; Intel Mac OS X 10_15_7", "X11; Linux x86_64"])
            })
            
            # Set a referer to make the request look more natural
            referrers = [
                "https://www.google.com/",
                "https://www.bing.com/",
                "https://www.linkedin.com/feed/",
                "https://www.indeed.com/",
                "https://www.glassdoor.com/"
            ]
            driver.execute_cdp_cmd("Network.setExtraHTTPHeaders", {
                "headers": {
                    "Referer": random.choice(referrers),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
                }
            })
            
            # Apply final window size
            driver.set_window_size(width, height)
            
            # Determine job site type for appropriate parsing strategy
            job_site_type = "generic"
            if 'indeed.com' in url.lower():
                job_site_type = "indeed"
            elif 'linkedin.com' in url.lower():
                job_site_type = "linkedin"
            elif 'glassdoor.com' in url.lower():
                job_site_type = "glassdoor"
            elif 'monster.com' in url.lower():
                job_site_type = "monster"
            elif 'ziprecruiter.com' in url.lower():
                job_site_type = "ziprecruiter"
                
            logger.info(f"Detected job site: {job_site_type}")
            
            # Extract job title hint from URL if available for enhancing the fallback
            job_title_hint = ""
            if any(param in url for param in ["keywords=", "q=", "search=", "query=", "title="]):
                try:
                    parsed_url = urlparse(url)
                    query_params = parse_qs(parsed_url.query)
                    
                    # Check multiple possible parameter names
                    for param in ["keywords", "q", "search", "query", "title"]:
                        if param in query_params and query_params[param][0]:
                            keywords = query_params[param][0]
                            job_title_hint = keywords.split()[0]  # Take first word as hint
                            logger.info(f"Extracted job title hint: {job_title_hint}")
                            break
                except Exception as e:
                    logger.warning(f"Error extracting job title hint: {e}")

            # Use our retry-enabled scraping function
            html_content = scrape_with_retry(driver, url, job_site_type)
            
            # Parse the content based on site type with multi-parsing strategy
            primary_jobs = []
            
            # Primary parsing strategy based on detected site type
            if 'indeed.com' in url.lower():
                logger.info("Using Indeed parser")
                primary_jobs = parse_indeed(html_content, url)
            elif 'linkedin.com' in url.lower():
                logger.info("Using LinkedIn parser")
                primary_jobs = parse_linkedin(html_content, url)
            else:
                logger.info("Using generic parser")
                primary_jobs = parse_generic(html_content, url)
            
            # If primary parser didn't yield results, try alternative parsers
            if len(primary_jobs) < 3 and retry_count > 1:
                logger.info(f"Primary parser only found {len(primary_jobs)} jobs, trying alternative parsers")
                
                if 'indeed.com' not in url.lower() and 'linkedin.com' not in url.lower():
                    # Try LinkedIn parser as fallback for unknown sites
                    logger.info("Trying LinkedIn parser as fallback")
                    fallback_jobs = parse_linkedin(html_content, url)
                    if len(fallback_jobs) > len(primary_jobs):
                        logger.info(f"LinkedIn parser found {len(fallback_jobs)} jobs, using these instead")
                        primary_jobs = fallback_jobs
                    
                # If still no good results, try generic parser for known sites
                if 'indeed.com' in url.lower() or 'linkedin.com' in url.lower():
                    if len(primary_jobs) < 2:
                        logger.info("Trying generic parser as last resort")
                        fallback_jobs = parse_generic(html_content, url)
                        if len(fallback_jobs) > len(primary_jobs):
                            logger.info(f"Generic parser found {len(fallback_jobs)} jobs, using these instead")
                            primary_jobs = fallback_jobs
            
            # Enhanced validation of job data
            valid_jobs = []
            ambiguous_jobs = []
            
            for job in primary_jobs:
                # First level validation - check if essential data exists
                has_title = job.get('title') and job.get('title') != "Unknown Title"
                has_company = job.get('company') and job.get('company') != "Unknown Company"
                
                # Second level validation - check for quality/specificity
                title_quality = False
                company_quality = False
                
                if has_title:
                    title = job.get('title', '')
                    # Check if title seems specific enough (not just generic terms)
                    generic_title_terms = ['position', 'job', 'listing', 'vacancy', 'opportunity']
                    title_quality = (len(title) >= 5 and
                                    not any(term == title.lower() for term in generic_title_terms) and
                                    not title.startswith(('Position ', 'Job ')))
                
                if has_company:
                    company = job.get('company', '')
                    # Check if company seems specific enough
                    generic_company_terms = ['company', 'employer', 'organization', 'recruiter']
                    company_quality = (len(company) >= 3 and
                                    not any(term == company.lower() for term in generic_company_terms) and
                                    not company.endswith((' Employer', ' Company')))
                
                # Categorize the job based on data quality
                if has_title and has_company and title_quality and company_quality:
                    # High quality job data
                    valid_jobs.append(job)
                elif has_title and has_company:
                    # Has required fields but quality might be questionable
                    ambiguous_jobs.append(job)
                else:
                    logger.warning(f"Skipping job with insufficient data: {job}")
                
            # Include ambiguous jobs only if we don't have enough valid ones
            if len(valid_jobs) < 3 and ambiguous_jobs:
                logger.info(f"Adding {len(ambiguous_jobs)} jobs with ambiguous quality to results")
                valid_jobs.extend(ambiguous_jobs)
                
            if len(valid_jobs) < len(primary_jobs):
                logger.warning(f"Filtered out {len(primary_jobs) - len(valid_jobs)} jobs with incomplete or poor quality data")
            
            # If we found valid jobs, return them
            if valid_jobs:
                logger.info(f"Successfully extracted {len(valid_jobs)} valid job listings")
                return valid_jobs
            
            # If no valid jobs, increment retry counter and try again
            logger.warning(f"No valid jobs found in attempt {retry_count+1}/{max_retries}")
            retry_count += 1
            
            if driver:
                driver.quit()
                driver = None
            
            time.sleep(2)  # Wait before retrying
            
        except Exception as e:
            logger.error(f"Error in scraping attempt {retry_count+1}: {str(e)}")
            logger.error(traceback.format_exc())
            retry_count += 1
            
            if driver:
                driver.quit()
                driver = None
                
            time.sleep(2)  # Wait before retrying
            
        finally:
            if driver:
                logger.info("Closing Chrome driver")
                driver.quit()
                driver = None
    
    # If we get here, all retries failed
    logger.error(f"Failed to extract valid jobs after {max_retries} attempts")
    return []
