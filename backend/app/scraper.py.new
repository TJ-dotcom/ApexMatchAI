from typing import List, Dict, Any
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import re
import logging
import requests

# Get module logger
logger = logging.getLogger("job_search_app.scraper")

def scrape_jobs(url: str) -> List[Dict[str, Any]]:
    """
    Scrape job listings from the given URL
    
    Args:
        url: URL of the job listing page
        
    Returns:
        List of dictionaries containing job details
    """
    logger.info(f"Starting job scraping from URL: {url}")
    
    try:
        # Configure Selenium options
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # Run in headless mode
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        
        logger.info("Setting up Chrome driver with headless options")
        
        # Set up the Chrome driver
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=chrome_options
        )
        
        # Navigate to the URL
        logger.info(f"Navigating to {url}")
        driver.get(url)
        
        # Wait for the page to load
        logger.info(f"Waiting for page to load")
        time.sleep(5)  # Simple wait for page to load
        
        # Get the HTML content
        html_content = driver.page_source
        logger.info(f"Retrieved HTML content, length: {len(html_content)} characters")
        
        # Check which job board we're scraping
        if 'indeed.com' in url.lower():
            logger.info("Detected Indeed job board, using Indeed parser")
            jobs = parse_indeed(html_content, url)
        elif 'linkedin.com' in url.lower():
            logger.info("Detected LinkedIn job board, using LinkedIn parser")
            jobs = parse_linkedin(html_content, url)
        else:
            logger.info("Unknown job board, using generic parser")
            jobs = parse_generic(html_content, url)
        
        logger.info(f"Scraping completed. Found {len(jobs)} job listings")
        return jobs
        
    except Exception as e:
        logger.error(f"Error scraping jobs: {str(e)}", exc_info=True)
        return []
    finally:
        if 'driver' in locals():
            logger.info("Closing Chrome driver")
            driver.quit()

def parse_indeed(html_content: str, base_url: str) -> List[Dict[str, Any]]:
    """Parse Indeed job listings"""
    logger.info("Starting Indeed parser")
    jobs = []
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        job_cards = soup.find_all('div', class_='job_seen_beacon')
        
        logger.info(f"Found {len(job_cards)} job cards on Indeed page")
        
        for i, card in enumerate(job_cards):
            try:
                # Extract job details
                title_elem = card.find('h2', class_='jobTitle')
                company_elem = card.find('span', class_='companyName')
                location_elem = card.find('div', class_='companyLocation')
                description_elem = card.find('div', class_='job-snippet')
                
                title = title_elem.text.strip() if title_elem else "Unknown Title"
                company = company_elem.text.strip() if company_elem else "Unknown Company"
                location = location_elem.text.strip() if location_elem else "Unknown Location"
                description = description_elem.text.strip() if description_elem else ""
                
                job_url = ""
                link = card.find('a', href=True)
                if link and 'href' in link.attrs:
                    job_url = f"https://www.indeed.com{link['href']}"
                
                jobs.append({
                    'title': title,
                    'company': company,
                    'location': location,
                    'description': description,
                    'url': job_url
                })
                
                logger.info(f"Parsed Indeed job {i+1}: {title} at {company}")
                
            except Exception as e:
                logger.error(f"Error parsing Indeed job card {i+1}: {str(e)}")
                continue
                
        return jobs
    except Exception as e:
        logger.error(f"Error in Indeed parser: {str(e)}", exc_info=True)
        return jobs

def parse_linkedin(html_content: str, base_url: str) -> List[Dict[str, Any]]:
    """Parse LinkedIn job listings"""
    logger.info("Starting LinkedIn parser")
    jobs = []
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        job_cards = soup.find_all('div', class_='base-search-card')
        
        logger.info(f"Found {len(job_cards)} job cards on LinkedIn page")
        
        for i, card in enumerate(job_cards):
            try:
                # Extract job details
                title_elem = card.find('h3', class_='base-search-card__title')
                company_elem = card.find('h4', class_='base-search-card__subtitle')
                location_elem = card.find('span', class_='job-search-card__location')
                description = ""  # LinkedIn doesn't show descriptions in the list view
                
                title = title_elem.text.strip() if title_elem else "Unknown Title"
                company = company_elem.text.strip() if company_elem else "Unknown Company"
                location = location_elem.text.strip() if location_elem else "Unknown Location"
                
                job_url = ""
                link = card.find('a', class_='base-card__full-link', href=True)
                if link and 'href' in link.attrs:
                    job_url = link['href']
                
                jobs.append({
                    'title': title,
                    'company': company,
                    'location': location,
                    'description': description,
                    'url': job_url
                })
                
                logger.info(f"Parsed LinkedIn job {i+1}: {title} at {company}")
                
            except Exception as e:
                logger.error(f"Error parsing LinkedIn job card {i+1}: {str(e)}")
                continue
                
        return jobs
    except Exception as e:
        logger.error(f"Error in LinkedIn parser: {str(e)}", exc_info=True)
        return jobs

def parse_generic(html_content: str, base_url: str) -> List[Dict[str, Any]]:
    """Generic parser for unknown job boards"""
    logger.info("Starting generic job board parser")
    jobs = []
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Look for common job listing patterns
        job_elements = (
            soup.find_all('div', class_=lambda c: c and ('job' in c.lower() or 'card' in c.lower())) or
            soup.find_all('li', class_=lambda c: c and ('job' in c.lower() or 'result' in c.lower())) or
            soup.find_all('div', class_=lambda c: c and ('listing' in c.lower() or 'result' in c.lower()))
        )
        
        logger.info(f"Found {len(job_elements)} potential job elements using generic parser")
        
        for i, element in enumerate(job_elements[:20]):  # Limit to first 20 to avoid junk
            try:
                # Try to find job details
                title_elem = (
                    element.find('h2') or 
                    element.find('h3') or 
                    element.find('a', class_=lambda c: c and 'title' in c.lower())
                )
                
                company_elem = (
                    element.find(string=re.compile('company', re.IGNORECASE)) or
                    element.find_all('span')[1] if len(element.find_all('span')) > 1 else None
                )
                
                location_elem = (
                    element.find(string=re.compile('location', re.IGNORECASE)) or
                    element.find(string=re.compile('address', re.IGNORECASE)) or
                    element.find_all('span')[2] if len(element.find_all('span')) > 2 else None
                )
                
                title = title_elem.text.strip() if title_elem else f"Job Listing {i+1}"
                company = company_elem.text.strip() if company_elem else "Company Not Found"
                location = location_elem.text.strip() if location_elem else "Location Not Found"
                description = element.get_text()[:200] + "..." if element else ""
                
                # Try to find a link
                job_url = ""
                link = element.find('a', href=True)
                if link and 'href' in link.attrs:
                    href = link['href']
                    if href.startswith('/'):
                        # Relative URL
                        from urllib.parse import urlparse
                        parsed_url = urlparse(base_url)
                        job_url = f"{parsed_url.scheme}://{parsed_url.netloc}{href}"
                    else:
                        job_url = href
                
                jobs.append({
                    'title': title,
                    'company': company,
                    'location': location,
                    'description': description,
                    'url': job_url
                })
                
                logger.info(f"Parsed generic job {i+1}: {title}")
                
            except Exception as e:
                logger.error(f"Error parsing generic job element {i+1}: {str(e)}")
                continue
        
        logger.info(f"Generic parsing completed, found {len(jobs)} jobs")
        return jobs
    except Exception as e:
        logger.error(f"Error in generic parser: {str(e)}", exc_info=True)
        return jobs
